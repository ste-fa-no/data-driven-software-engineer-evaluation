{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction to the task\n",
    "\n",
    "The task consists in extracting usable information from documents containing text written in natural language.\n",
    "\n",
    "### Quick setup guide\n",
    "\n",
    "Follow these steps to set up and run the project:\n",
    "- ensure `Python` (version `3.7` or newer) is installed and added to `PATH`.\n",
    "- clone this repository using Git:\n",
    "    ```shell\n",
    "    git clone <repository-url>\n",
    "    ```\n",
    "- navigate to the project folder and run the setup script\n",
    "    ```shell\n",
    "    python setup.py\n",
    "    ```\n",
    "- run tests to verify setup (make sure you are inside the data-processing folder)\n",
    "    ```shell\n",
    "    python -m unittest discover\n",
    "    ```\n",
    "\n",
    "### A closer look at the domain of the problem\n",
    "\n",
    "The candidate is required to figure what kind of information would be useful to extract, as no explanation is given in the `README`.\n",
    "Looking through the sample documents provided in the `pdfs` folder, you can see that the entirety of their content is a placeholder.\n",
    "Therefore, I'll assume that no information needs to be extracted from the paragraphs, as they would contain actual content otherwise.\n",
    "This means that the scope narrows down to the sole 'captions' and 'labels' containing information that is already blatantly meaningful.\n",
    "\n",
    "### Getting started\n",
    "\n",
    "A first approach involves natural language processing.\n",
    "There are pre-trained models that can do named entity recognition.\n",
    "In this demonstration, I will use the `spaCy` module to get up-and-running quickly.\n",
    "\n",
    "First off, I want to extract text from a pdf document.\n",
    "I will use `PyPDF2` for this.\n"
   ],
   "id": "8f3863006b5b50be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import docproc as dp\n",
    "from spacy import Language\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sample3 = dp.load_file('./pdfs/sample-3.pdf')\n",
    "sample3_text = dp.get_document_text(sample3)\n",
    "\n",
    "print(sample3_text)"
   ],
   "id": "1d42928393f6647c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that, in some cases, information can be useless when it's extracted away from its context.\n",
    "In order to avoid a complete (or partial) loss of semantics in the data that we extract, we can split it into chunks.\n",
    "\n",
    "Assuming that documents contain a single macro-topic an approach like this is probably exaggerated, since most sentences will likely share similar context regardless.\n",
    "With `spaCy`, we have an easy way to split our documents into sentences, which should be enough for our purpose."
   ],
   "id": "8b97a538b5350b28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nlp: Language = spacy.load('en_core_web_trf')\n",
    "\n",
    "sentences = dp.get_sentences(nlp, sample3_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence, end='\\n\\n')"
   ],
   "id": "40e3b3e2d2305885",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Once we have the document split in sentences, we can proceed extracting individual features.\n",
    "The following example extracts all the dates in the document, sentence by sentence."
   ],
   "id": "de606f6828a1c445"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for n, sentence in enumerate(dp.get_sentences(nlp, sample3_text)):\n",
    "    print(f'Sentence {n}:')\n",
    "    print('\\tDates:')\n",
    "    for date in dp.extract_dates(nlp, sentence.text):\n",
    "        print('\\t\\t', date, end='\\n\\n')"
   ],
   "id": "13d50ae5edecdcc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Out-of-the-box experience limitations with spaCy and their pre-trained models\n",
    "\n",
    "There are two fundamental limitations when it comes to using the default configurations that spaCy provides:\n",
    "- the sentence recognition is done by a pre-trained model that takes grammar into account. This is more accurate than the rule-based alternative `Sentencizer`, but both fall short when dealing with languages the model is unfamiliar with (e.g. the pseudo-latin of Lorem Ipsum).\n",
    "- the named entity recognition only supports 18 types of entities by default. Although it is possible to train a custom NER pipeline, the process requires a considerable amount of good quality data.\n",
    "\n",
    "### Enter regular expressions\n",
    "\n",
    "Regular expressions are handy when handling data that matches a certain pattern.\n",
    "For example, `spaCy` cannot recognize email addresses by default.\n",
    "\n",
    "With regex, it is just a matter of finding the tokens that have the structure of an email address.\n",
    "Here's an updated version of the code above that can also extract email addresses."
   ],
   "id": "817fa2e26934b865"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for n, sentence in enumerate(dp.get_sentences(nlp, sample3_text)):\n",
    "    print(f'Sentence {n}:')\n",
    "    print('\\tDates:')\n",
    "    for date in dp.extract_dates(nlp, sentence.text):\n",
    "        print('\\t\\t', date)\n",
    "    print()\n",
    "    print('\\tEmail Addresses:')\n",
    "    for addr in dp.extract_emails(nlp, sentence.text):\n",
    "        print('\\t\\t', addr, end='\\n\\n')"
   ],
   "id": "4ba05094d6386bf6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
